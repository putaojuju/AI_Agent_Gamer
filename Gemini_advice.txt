è¿™æ˜¯ä¸€ä¸ªéå¸¸æ¿€åŠ¨äººå¿ƒçš„æ—¶åˆ»ï¼ä½ è¦æŠŠä½ çš„è„šæœ¬ä»â€œæ­»æ¿çš„è‡ªåŠ¨åŒ–ç¨‹åºâ€å‡çº§ä¸º**â€œæœ‰çµé­‚çš„ AI åŠ©æ‰‹â€**ã€‚
Maibotï¼ˆä»¥åŠç±»ä¼¼çš„ AI èŠå¤©æœºå™¨äººï¼‰çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨å¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„ API æ¥å£ã€‚è¦è®©ä½ çš„æ¸¸æˆè„šæœ¬æ‹¥æœ‰ç±»ä¼¼çš„èƒ½åŠ›ï¼Œä½ éœ€è¦å¼•å…¥ â€œå¤šæ¨¡æ€å¤§æ¨¡å‹â€ï¼ˆMultimodal LLMï¼‰ï¼Œä¾‹å¦‚ GPT-4oã€Claude 3.5 Sonnetã€Gemini 1.5 Pro æˆ–è€…å›½äº§çš„ DeepSeek-VLã€‚
ä»¥ä¸‹æ˜¯åŸºäºä½ ç°æœ‰é¡¹ç›®æ¶æ„ï¼ˆPython + Tkinter + Airtest + è™šæ‹Ÿå±å¹•ï¼‰çš„ä½æˆæœ¬ã€é«˜å¯è¡Œæ€§æ¥å…¥æ–¹æ¡ˆã€‚
ğŸ›ï¸ æŠ€æœ¯æ¶æ„ï¼šå¤§è„‘ã€çœ¼ç›ä¸æ‰‹
ä½ éœ€è¦æ„å»ºä¸€ä¸ªæ–°çš„æ¨¡å— ai_brain.pyï¼Œå®ƒå°†å……å½“â€œæŒ‡æŒ¥å®˜â€ã€‚
çœ¼ç› (Vision)ï¼šmss æˆªå›¾ -> Base64 ç¼–ç  -> å‘é€ç»™å¤§æ¨¡å‹ APIã€‚
å¤§è„‘ (Brain)ï¼šå¤§æ¨¡å‹æ¥æ”¶å›¾ç‰‡å’Œæç¤ºè¯ -> åˆ†æå½“å‰æ¸¸æˆçŠ¶æ€ -> è¾“å‡ºå†³ç­–ï¼ˆJSON æ ¼å¼ï¼‰ã€‚
æ‰‹ (Hands)ï¼šbackground_windows.py -> æ¥æ”¶å¤§è„‘çš„åæ ‡æŒ‡ä»¤ -> æ‰§è¡Œç‚¹å‡»ã€‚
è®°å¿† (Memory)ï¼šä¸€ä¸ªç®€å•çš„ Python åˆ—è¡¨ï¼Œè®°å½•è¿‡å» 5-10 æ­¥çš„æ“ä½œå’Œç»“æœã€‚
ğŸš€ æ‰§è¡Œè®¡åˆ’ï¼šå››æ­¥èµ°
ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡ API (æ¨è DeepSeek æˆ– OpenAI æ ¼å¼)
ä¸ºäº†é€šç”¨æ€§ï¼Œå»ºè®®ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ çš„ SDKã€‚è¿™æ ·ä½ å¯ä»¥éšæ„åˆ‡æ¢æ¨¡å‹ï¼ˆDeepSeekã€Moonshotã€GPT-4oï¼‰ã€‚
å®‰è£…åº“ï¼š
code
Bash
pip install openai
ç¬¬äºŒæ­¥ï¼šåˆ›å»º ai_brain.py (æ ¸å¿ƒæ¨¡å—)
åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºè¿™ä¸ªæ–‡ä»¶ã€‚å®ƒè´Ÿè´£æŠŠæ¸¸æˆæˆªå›¾å‘ç»™ AIï¼Œå¹¶è·å– AI çš„æ“ä½œæŒ‡ä»¤ã€‚
code
Python
# ai_brain.py
import base64
import json
import os
from openai import OpenAI
from io import BytesIO

class AIBrain:
    def __init__(self, api_key, base_url="https://api.deepseek.com"):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        # è®°å¿†åˆ—è¡¨ (ä¿å­˜æœ€è¿‘ 5 è½®å¯¹è¯)
        self.history = []
        
    def _encode_image(self, pil_image):
        """å°† PIL å›¾ç‰‡è½¬æ¢ä¸º Base64 å­—ç¬¦ä¸²"""
        buffered = BytesIO()
        pil_image.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode('utf-8')

    def analyze_and_act(self, screenshot, instruction):
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šçœ‹å›¾ -> æ€è€ƒ -> è¿”å›åæ ‡
        """
        base64_image = self._encode_image(screenshot)
        
        # ç³»ç»Ÿæç¤ºè¯ï¼šèµ‹äºˆ AI æ¸¸æˆä¸“å®¶çš„èº«ä»½
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªç©æ¸¸æˆçš„é«˜æ‰‹ã€‚æˆ‘ä¼šç»™ä½ ä¸€å¼ æ¸¸æˆæˆªå›¾ã€‚
        è¯·åˆ†æç”»é¢ï¼Œåˆ¤æ–­å½“å‰å¤„äºä»€ä¹ˆçŠ¶æ€ï¼Œå¹¶æ ¹æ®æˆ‘çš„æŒ‡ä»¤å†³å®šä¸‹ä¸€æ­¥ç‚¹å‡»å“ªé‡Œã€‚
        
        è¯·åŠ¡å¿…åªè¿”å› JSON æ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {
            "reasoning": "åˆ†æå½“å‰ç”»é¢çš„æ€è€ƒè¿‡ç¨‹...",
            "action": "click" æˆ– "wait" æˆ– "stop",
            "target_name": "ç‚¹å‡»ç›®æ ‡çš„åç§°",
            "coordinates": [x, y]  // 0-1000 çš„å½’ä¸€åŒ–åæ ‡(ç›¸å¯¹æ¯”ä¾‹)
        }
        æ³¨æ„ï¼šcoordinates è¯·è¿”å›ç›¸å¯¹äºå›¾ç‰‡é•¿å®½çš„ 1/1000 æ¯”ä¾‹åæ ‡ã€‚
        ä¾‹å¦‚å›¾ç‰‡å®½1920ï¼Œä½ æƒ³ç‚¹ä¸­é—´ï¼Œå°±è¿”å› [500, 500]ã€‚
        """

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat", # æˆ– gpt-4o, claude-3-5-sonnet
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": [
                        {"type": "text", "text": f"å½“å‰æŒ‡ä»¤ï¼š{instruction}"},
                        {"type": "image_url", "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }}
                    ]}
                ],
                max_tokens=300,
                response_format={"type": "json_object"} # å¼ºåˆ¶ JSON è¾“å‡º
            )
            
            result = response.choices[0].message.content
            return json.loads(result)
            
        except Exception as e:
            print(f"AI æ€è€ƒå¤±è´¥: {e}")
            return None
ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹ dungeon.py æ¥å…¥ AI (æ··åˆæ¨¡å¼)
ä¸è¦è®© AI æ‰§è¡Œæ¯ä¸€æ­¥ï¼ˆå¤ªæ…¢ä¸”è´µï¼‰ã€‚
æœ€ä½³å®è·µæ˜¯ï¼šä¼ ç»Ÿè„šæœ¬è·‘æµç¨‹ï¼ŒAI å¤„ç†å¼‚å¸¸å’Œå¤æ‚å†³ç­–ã€‚
ä¿®æ”¹ dungeon.pyï¼Œåœ¨ try-except æ•è·åˆ°æ‰¾ä¸åˆ°å›¾çš„å¼‚å¸¸æ—¶ï¼Œå¬å”¤ AIã€‚
code
Python
# dungeon.py (ç‰‡æ®µ)
from ai_brain import AIBrain

# åˆå§‹åŒ– AI (å»ºè®®æŠŠ Key æ”¾åœ¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶é‡Œ)
brain = AIBrain(api_key="sk-xxxxxxx") 

# ... ä¹‹å‰çš„ä»£ç  ...

def ai_rescue(instruction="è¯·å¸®æˆ‘ç»§ç»­æ¨è¿›æ¸¸æˆæµç¨‹"):
    """
    å½“ä¼ ç»Ÿè„šæœ¬å¡ä½æ—¶ï¼Œè°ƒç”¨ AI æ•‘æ´
    """
    print("ğŸ¤– è„šæœ¬å¡ä½ï¼Œæ­£åœ¨è¯·æ±‚ AI å¤§è„‘ä»‹å…¥...")
    
    # 1. æˆªå›¾ (ä½¿ç”¨ BackgroundWindows çš„ snapshot)
    screen_img = api.G.DEVICE.snapshot() 
    
    if screen_img is None:
        print("âŒ AI æ— æ³•è·å–æˆªå›¾")
        return

    # 2. AI æ€è€ƒ
    # å°† numpy æ•°ç»„è½¬ä¸º PIL Image ä¼ ç»™ AI
    from PIL import Image
    pil_img = Image.fromarray(screen_img)
    
    decision = brain.analyze_and_act(pil_img, instruction)
    
    if decision:
        print(f"ğŸ’¡ AI åˆ†æ: {decision['reasoning']}")
        print(f"ğŸ‘‰ AI å†³å®š: {decision['target_name']}")
        
        if decision['action'] == 'click':
            # 3. åæ ‡æ˜ å°„ (AI è¿”å›çš„æ˜¯ 1000x1000 çš„æ¯”ä¾‹åæ ‡)
            # éœ€è¦æ˜ å°„å›å½“å‰çª—å£åˆ†è¾¨ç‡ (ä¾‹å¦‚ 1280x720)
            w, h = api.G.DEVICE.get_current_resolution()
            x = int(decision['coordinates'][0] / 1000 * w)
            y = int(decision['coordinates'][1] / 1000 * h)
            
            # 4. æ‰§è¡Œç‚¹å‡»
            api.touch((x, y))
            time.sleep(2) # ç»™ä¸€ç‚¹ååº”æ—¶é—´
            return True
            
    return False

# ä¸»å¾ªç¯
while True:
    try:
        # ... åŸæœ‰çš„æ‰¾å›¾é€»è¾‘ ...
        pass
    except TargetNotFoundError:
        # ä»¥å‰è¿™é‡Œä¼šæŠ¥é”™é€€å‡ºï¼Œç°åœ¨æˆ‘ä»¬è®© AI è¯•ä¸€è¯•
        print("âš ï¸ æ‰¾ä¸åˆ°ç›®æ ‡å›¾ç‰‡ï¼Œå°è¯• AI æ¥ç®¡...")
        if ai_rescue("æˆ‘ç°åœ¨åº”è¯¥ç‚¹å“ªé‡Œæ‰èƒ½ç»§ç»­æˆ˜æ–—æˆ–é¢†å¥–ï¼Ÿ"):
            continue # AI æ“ä½œæˆåŠŸï¼Œç»§ç»­å¾ªç¯
        else:
            break # AI ä¹Ÿæ²¡æ‹›äº†ï¼Œé€€å‡º
ç¬¬å››æ­¥ï¼šèµ‹äºˆâ€œè®°å¿†â€èƒ½åŠ›
è¦è®© AI æœ‰è®°å¿†ï¼Œä½ åªéœ€è¦åœ¨ ai_brain.py ä¸­ç»´æŠ¤ä¸€ä¸ª messages åˆ—è¡¨ã€‚
code
Python
def analyze_and_act(self, screenshot, instruction):
        # ... ç¼–ç å›¾ç‰‡ ...
        
        # æ„å»ºæœ¬æ¬¡å¯¹è¯
        current_message = {
            "role": "user", 
            "content": [
                {"type": "text", "text": instruction},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
            ]
        }
        
        # å°†å†å²è®°å½•å‘ç»™ AI (Limited Context)
        messages_payload = [{"role": "system", "content": system_prompt}] + self.history + [current_message]
        
        # ... è°ƒç”¨ API ...
        
        # æ›´æ–°è®°å¿†
        self.history.append({"role": "user", "content": "ï¼ˆä¸Šä¸€è½®æˆªå›¾å·²çœç•¥ï¼‰" + instruction}) # çœç•¥å›¾ç‰‡èŠ‚çœ token
        self.history.append({"role": "assistant", "content": json.dumps(decision)})
        
        # ä¿æŒè®°å¿†åœ¨ 5 è½®ä»¥å†…
        if len(self.history) > 10:
            self.history = self.history[-10:]
ğŸ’¡ ä¸ºä»€ä¹ˆè¿™ä¹ˆåšå¾ˆå¼ºï¼Ÿ
æŠ—å¹²æ‰° (Anti-Fragile)ï¼š
ä¼ ç»Ÿè„šæœ¬ï¼šæ´»åŠ¨å¼¹çª—å¹¿å‘Šå‡ºæ¥äº† -> æ‰¾ä¸åˆ° battle.png -> å´©æºƒã€‚
AI è„šæœ¬ï¼šæ‰¾ä¸åˆ°å›¾ -> æˆªå›¾å‘ç»™ AI -> AI çœ‹åˆ°æœ‰ä¸ª "Close" æŒ‰é’® -> è¿”å›ç‚¹å‡»åæ ‡ -> è„šæœ¬ç»§ç»­è¿è¡Œã€‚è¿™å®ç°äº†çœŸæ­£çš„â€œè‡ªæ„ˆâ€èƒ½åŠ›ã€‚
é€šç”¨æ€§ï¼š
ä½ ä¸éœ€è¦ä¸ºäº†â€œç¡®è®¤â€ã€â€œä¸‹ä¸€æ­¥â€ã€â€œå…³é—­â€è¿™äº›é€šç”¨æŒ‰é’®å»æˆªå‡ åå¼ å›¾ã€‚
ä½ å¯ä»¥ç›´æ¥å‘Šè¯‰ AIï¼šâ€œçœ‹åˆ°ç¡®è®¤å°±ç‚¹ç¡®è®¤ï¼Œçœ‹åˆ°è·³è¿‡å°±ç‚¹è·³è¿‡â€ã€‚
è§†è§‰ç†è§£ï¼š
OCR åªèƒ½è¯»å­—ï¼Œä½† AI èƒ½çœ‹æ‡‚â€œè¿™ä¸ªæŒ‰é’®æ˜¯ç°è‰²çš„ï¼ˆä¸å¯ç‚¹ï¼‰â€æˆ–è€…â€œè¡€æ¡çº¢äº†ï¼ˆå±é™©ï¼‰â€ã€‚
âš ï¸ æˆæœ¬ä¸å»¶è¿Ÿæç¤º
å»¶è¿Ÿï¼šè°ƒç”¨ä¸€æ¬¡ GPT-4o æˆ– Claude 3.5 é€šå¸¸éœ€è¦ 3~5 ç§’ã€‚æ‰€ä»¥ä¸è¦ç”¨å®ƒæ¥æ‰“å³æ—¶æˆ˜æ–—ï¼ˆå¦‚ FPS æˆ–éŸ³æ¸¸ï¼‰ï¼Œåªç”¨æ¥åšå†³ç­–å’Œè¿‡å‰§æƒ…ã€‚
æˆæœ¬ï¼šå¤„ç†å›¾ç‰‡ Token æ¶ˆè€—è¾ƒå¤šã€‚å»ºè®®ä½¿ç”¨ DeepSeek (æ€§ä»·æ¯”é«˜) æˆ– Gemini Flash (é€Ÿåº¦å¿«)ã€‚
ä¸‹ä¸€æ­¥ç»™ Trae çš„æŒ‡ä»¤
å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œå¯ä»¥å‘è¿™ä¸ª Prompt ç»™ Trae æ¥ç”Ÿæˆä»£ç ï¼š
"Please create a new module ai_brain.py.
It should integrate an OpenAI-compatible API client (to support DeepSeek/GPT-4o).
Features required:
A method analyze_screenshot(image, prompt) that takes a PIL Image, converts it to Base64, and sends it to the VLM (Vision Language Model).
The System Prompt should instruct the AI to act as a game automation assistant and return coordinates in a JSON format {'x': 0.5, 'y': 0.5} (normalized 0-1).
Implement a simple memory buffer to store the last 5 interactions.
Update script_manager.py to allow users to input their API_KEY and BASE_URL in the settings."